{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4b0770c-e031-4771-86e3-bdf6c27d2010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/wiizh92/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "## utility functions ##\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "def to_data(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cpu()\n",
    "    return x.data.numpy()\n",
    "\n",
    "def to_var(x):\n",
    "    if not torch.is_tensor(x):\n",
    "        x = torch.Tensor(x)\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return x\n",
    "\n",
    "def display_dialog_history(dialog_hx):\n",
    "    for j, line in enumerate(dialog_hx):\n",
    "        msg = tokenizer.decode(line)\n",
    "        if j %2 == 0:\n",
    "            print(\">> User: \"+ msg)\n",
    "        else:\n",
    "            print(\"Bot: \"+msg)\n",
    "            print()\n",
    "\n",
    "def generate_next(bot_input_ids, do_sample=True, top_k=10, top_p=.92,\n",
    "                  max_length=1000, pad_token=tokenizer.eos_token_id):\n",
    "    full_msg = model.generate(bot_input_ids, do_sample=True,\n",
    "                                              top_k=top_k, top_p=top_p, \n",
    "                                              max_length=max_length, pad_token_id=tokenizer.eos_token_id)\n",
    "    msg = to_data(full_msg.detach()[0])[bot_input_ids.shape[-1]:]\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42ef8329-4b38-4b26-a401-6dd9389403a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">> Fact 1:  Name: Emma Brown Age: 20 Nationality: American Languages Spoken: English, Spanish\n",
      ">> Fact 2:  Hobbies: Enjoys reading mystery novels. Loves painting and sketching. Plays the guitar in her free time.\n",
      ">> Fact 3:  Education: Currently studying Psychology at university. Interested in learning about human behavior and mental health.\n",
      ">> Fact 4:  Lifestyle: Likes to stay active by jogging and practicing yoga. Enjoys cooking and trying out new recipes.\n",
      ">> Fact 5:  Social Life: Active on social media and loves sharing her artwork. Participates in university clubs, especially the art and music clubs.\n"
     ]
    }
   ],
   "source": [
    "personas = []\n",
    "for i in range(5):\n",
    "    response = input(\">> Fact %d: \"%(i+1))+ tokenizer.eos_token\n",
    "    personas.append(response)\n",
    "personas = tokenizer.encode(''.join(['<|p2|>'] + personas + ['<|sep|>'] + ['<|start|>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb09b669-8531-4069-a3e4-01f46e082df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">> User:   Hi, what is your name and tell me about yourself.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Your name :D\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">> User:  what do you like to do?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: I'll PM\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">> User:  what do you study?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: I love : I love to\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">> User:  what language do you speak?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: You're\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">> User:  how old are you?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: I don't know about yourself\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">> User:  what do you like to do when you are not working?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">> User:  Does money buy happiness?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">> User:  This is so difficult !\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: \n"
     ]
    }
   ],
   "source": [
    "# converse for 8 turns\n",
    "dialog_hx = []\n",
    "for step in range(8):\n",
    "    # encode the user input\n",
    "    user_inp = tokenizer.encode(input(\">> User: \") + tokenizer.eos_token)\n",
    "    # append to the chat history\n",
    "    dialog_hx.append(user_inp)\n",
    "        \n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    bot_input_ids = to_var([personas + flatten(dialog_hx)]).long()\n",
    "    msg = generate_next(bot_input_ids)\n",
    "    dialog_hx.append(msg)\n",
    "    print(\"Bot: {}\".format(tokenizer.decode(msg, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bff132-6389-45c0-8952-52718df3a946",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
